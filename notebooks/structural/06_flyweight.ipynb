{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural Patterns\n",
    "\n",
    "The creational patterns are guidelines suggested to compose objects in a way that allows to perform new functionalities. Those new functionalities would be harder to do without the new class/function created.\n",
    "\n",
    "The structural patterns at the same time allows dealing with across entities relationships in an easier and managable way.\n",
    "\n",
    "In simple words: **How to compose complex objects**\n",
    "\n",
    "The seven structural patterns available are:\n",
    "1. Adapter\n",
    "2. Bridge\n",
    "3. Composite\n",
    "4. Decorator\n",
    "5. Facade\n",
    "6. Flyweight\n",
    "7. Proxy\n",
    "\n",
    "Each of those previous patterns are unique, and can be used in different situations and conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flyweight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the flyweight?**\n",
    "\n",
    "This is a design pattern that suggest to classify the inner states of an object between intrinsic and extrinsic states. The **intrinsic states** are common states between across you're code, they're suggested to cached. On the other hand, the **extrinsic states** change over the execution, so it is not worth to save all of them. If you're saving you're extrinsic states, you're likely to run out of memory during your program execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When should we use it?**\n",
    "\n",
    "This is suggested to use when our code execution has common steps that can be cached to speed up execution. This will serve like a feature store to provide a way for downstream executions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario**\n",
    "\n",
    "You need to create from scratch an AutoML pipeline that process the data and trains a random forest, a linear model, and a xgboost model. At the end, this should be printing the metrics of each model trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_diabetes(as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df[0], df[1]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an split for train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"target\"]\n",
    "x = df.drop([\"target\"], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.3, random_state=99\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antipattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest antipattern to reproduce is the one which executes everything from scratch when training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this an antipattern? We introduce an overhead of processing because for all models trained, we will use the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our intrinsic states are driven by the data processing until before the model training. Why? All the data until this point will be the same. \n",
    "\n",
    "What happens after this point? Well, from this point we're reaching the extrinsic states. Each of the possible models can take different configurations and everytime we're training a model, this might change even due to a generator seed given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with out antipattern class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoMLPipeline:\n",
    "    def __init__(self, model_names: list):\n",
    "        self.model_names = model_names\n",
    "        self.model_instances = {}\n",
    "\n",
    "    def fit(self, x_train: pd.DataFrame, y_train: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fits the models for the AutoML pipeline\n",
    "        \"\"\"\n",
    "        x_train = x_train.copy()\n",
    "        for model in self.model_names:\n",
    "            self.model_instances[model] = self.__fit_model(model, x_train, y_train)\n",
    "\n",
    "    def __fit_model(self, model_name: str, x_train: pd.DataFrame, y_train: pd.Series):\n",
    "        \"\"\"\n",
    "        Fits a model based on the model name provided.\n",
    "        \"\"\"\n",
    "        x_train = self._minmax_scaler(x_train)\n",
    "        if model_name == \"rf\":\n",
    "            return self._train_rf(x_train, y_train)\n",
    "        elif model_name == \"lm\":\n",
    "            return self._train_lm(x_train, y_train)\n",
    "        elif model_name == \"xgboost\":\n",
    "            return self._train_xgboost(x_train, y_train)\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} not recognized.\")\n",
    "\n",
    "    def _minmax_scaler(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scales all columns given a MinMaxScaler instance.\n",
    "        \"\"\"\n",
    "        scaler = MinMaxScaler()\n",
    "        df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "        return df\n",
    "\n",
    "    def _train_rf(self, x_train: pd.DataFrame, y_train: pd.Series):\n",
    "        \"\"\"\n",
    "        Trains a RandomForestRegressor model.\n",
    "        \"\"\"\n",
    "        model = RandomForestRegressor()\n",
    "        model.fit(x_train, y_train)\n",
    "        return model\n",
    "\n",
    "    def _train_lm(self, x_train: pd.DataFrame, y_train: pd.Series):\n",
    "        \"\"\"\n",
    "        Trains a LinearRegression model.\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(x_train, y_train)\n",
    "        return model\n",
    "\n",
    "    def _train_xgboost(self, x_train: pd.DataFrame, y_train: pd.Series):\n",
    "        \"\"\"\n",
    "        Trains an XGBRegressor model.\n",
    "        \"\"\"\n",
    "        from xgboost import XGBRegressor\n",
    "\n",
    "        model = XGBRegressor()\n",
    "        model.fit(x_train, y_train)\n",
    "        return model\n",
    "\n",
    "    def get_scores(self, x_test: pd.DataFrame, y_test: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Gets MAE for each model trained\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        for model in self.model_names:\n",
    "            y_pred = self.model_instances[model].predict(x_test)\n",
    "            scores[model] = mae(y_true=y_test, y_pred=y_pred)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the pipeline, but let's create a timer to count the time-needed for the execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "automl = AutoMLPipeline(model_names=[\"rf\", \"lm\", \"xgboost\"])\n",
    "automl.fit(x_train, y_train)\n",
    "end_time = time.time()\n",
    "scores = automl.get_scores(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24162721633911133\n"
     ]
    }
   ],
   "source": [
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw previously, it took around 0.21 seconds. It is less than a second, but if we start to increase the preprocessing steps into our AutoML class, this will be several times longer. Even though we can't see right now a big problem, if the pipeline grows up int the future, this will take a long time for execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to solve the antipattern?**\n",
    "\n",
    "1. Perform the data processing outside the each model training (extrinsic -> instrinsic).\n",
    "2. Cache the results of the preprocessing.\n",
    "3. Use the cached version of data for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new class with the modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewAutoMLPipeline:\n",
    "    def __init__(self, model_names: list):\n",
    "        self.model_names = model_names\n",
    "        self.model_instances = {}\n",
    "        self.df_cache = None\n",
    "\n",
    "    def fit(self, x_train: pd.DataFrame, y_train: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fits the models for the AutoML pipeline\n",
    "        \"\"\"\n",
    "        x_train = x_train.copy()\n",
    "        self.df_cache = self._minmax_scaler(x_train)\n",
    "        for model in self.model_names:\n",
    "            self.model_instances[model] = self.__fit_model(\n",
    "                model, self.df_cache, y_train\n",
    "            )\n",
    "\n",
    "    def __fit_model(self, model_name: str, x_train: pd.DataFrame, y_train: pd.Series):\n",
    "        \"\"\"\n",
    "        Fits a model based on the model name provided.\n",
    "        \"\"\"\n",
    "        if model_name == \"rf\":\n",
    "            return self._train_rf(x_train, y_train)\n",
    "        elif model_name == \"lm\":\n",
    "            return self._train_lm(x_train, y_train)\n",
    "        elif model_name == \"xgboost\":\n",
    "            return self._train_xgboost(x_train, y_train)\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} not recognized.\")\n",
    "\n",
    "    def _minmax_scaler(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scales all columns given a MinMaxScaler instance.\n",
    "        \"\"\"\n",
    "        scaler = MinMaxScaler()\n",
    "        df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "        return df\n",
    "\n",
    "    def _train_rf(self, x_train: pd.DataFrame, y_train: pd.Series):\n",
    "        \"\"\"\n",
    "        Trains a RandomForestRegressor model.\n",
    "        \"\"\"\n",
    "        model = RandomForestRegressor()\n",
    "        model.fit(x_train, y_train)\n",
    "        return model\n",
    "\n",
    "    def _train_lm(self, x_train: pd.DataFrame, y_train: pd.Series):\n",
    "        \"\"\"\n",
    "        Trains a LinearRegression model.\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(x_train, y_train)\n",
    "        return model\n",
    "\n",
    "    def _train_xgboost(self, x_train: pd.DataFrame, y_train: pd.Series):\n",
    "        \"\"\"\n",
    "        Trains an XGBRegressor model.\n",
    "        \"\"\"\n",
    "        model = XGBRegressor()\n",
    "        model.fit(x_train, y_train)\n",
    "        return model\n",
    "\n",
    "    def get_scores(self, x_test: pd.DataFrame, y_test: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Gets MAE for each model trained\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        for model in self.model_names:\n",
    "            y_pred = self.model_instances[model].predict(x_test)\n",
    "            scores[model] = mae(y_true=y_test, y_pred=y_pred)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "automl = NewAutoMLPipeline(model_names=[\"rf\", \"lm\", \"xgboost\"])\n",
    "automl.fit(x_train, y_train)\n",
    "end_time = time.time()\n",
    "scores = automl.get_scores(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21244382858276367\n"
     ]
    }
   ],
   "source": [
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw before, we reduced some miliseconds of preprocessing (around 0.03). However, if this is exposed to bigger setups or datasets, this is where the flyweigths scenario will bright!!! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
